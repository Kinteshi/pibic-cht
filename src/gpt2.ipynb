{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import io\n","import os\n","import torch\n","from tqdm.notebook import tqdm\n","from torch.utils.data import Dataset, DataLoader\n","# from ml_things import plot_dict, plot_confusion_matrix, fix_text\n","from sklearn.metrics import classification_report, accuracy_score, f1_score\n","from transformers import (set_seed,\n","                          TrainingArguments,\n","                          Trainer,\n","                          GPT2Config,\n","                          GPT2Tokenizer,\n","                          AdamW,\n","                          get_linear_schedule_with_warmup,\n","                          GPT2ForSequenceClassification)\n","import numpy as np\n","import pandas as pd\n","from sklearn.utils import resample\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","\n","seed = 15\n","epochs = 4\n","batch_size = 16\n","max_length = 150\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model_name_or_path = 'pierreguillou/gpt2-small-portuguese'\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df = pd.read_csv('C:\\\\Users\\\\jefma\\\\GitHub\\\\pibic-cht\\\\data\\\\tceTextData.csv')\n","df.columns = ['empenho', 'natureza']\n","df = get_topN_labels_doc(df, 'natureza', 400)\n","\n","n_samples = int(df.values.shape[0] * 0.33)\n","newdf = resample(df, n_samples=n_samples,\n","                 random_state=seed, stratify=df.natureza)\n","newdf.empenho = newdf.empenho.apply(embeddingPrep)\n","\n","le = LabelEncoder()\n","df['encodedNatureza'] = le.fit_transform(df.natureza)\n","n_labels = len(le.classes_)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","class TCEDataset(Dataset):\n","    def __init__(self, empenho, targets, tokenizer, max_len):\n","        self.empenho = empenho\n","        self.targets = targets\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.empenho)\n","\n","    def __getitem__(self, item):\n","        return {\n","            'text': str(self.empenho[item]),\n","            'label': self.targets[item]\n","        }\n","\n","\n","def create_data_loader(df, tokenizer, max_len, batch_size, collator):\n","    ds = TCEDataset(\n","        empenho=df.empenho.to_numpy(),\n","        targets=df.encodedNatureza.to_numpy(),\n","        tokenizer=tokenizer,\n","        max_len=max_len\n","    )\n","    return DataLoader(\n","        ds,\n","        batch_size=batch_size,\n","        collate_fn=collator\n","    )\n","\n","\n","class Gpt2ClassificationCollator(object):\n","\n","    def __init__(self, use_tokenizer, max_sequence_len=None):\n","\n","        self.use_tokenizer = use_tokenizer\n","        self.max_sequence_len = use_tokenizer.model_max_length if max_sequence_len is None else max_sequence_len\n","        return\n","\n","    def __call__(self, sequences):\n","\n","        texts = [sequence['text'] for sequence in sequences]\n","        labels = [sequence['label'] for sequence in sequences]\n","        inputs = self.use_tokenizer(text=texts, return_tensors=\"pt\",\n","                                    padding=True, truncation=True,  max_length=self.max_sequence_len)\n","        inputs.update({'labels': torch.tensor(labels)})\n","\n","        return inputs\n","\n","\n","def train(dataloader, optimizer_, scheduler_, device_):\n","\n","    global model\n","\n","    predictions_labels = []\n","    true_labels = []\n","    total_loss = 0\n","\n","    model.train()\n","\n","    for batch in tqdm(dataloader, total=len(dataloader)):\n","        true_labels += batch['labels'].numpy().flatten().tolist()\n","        batch = {k: v.type(torch.long).to(device_) for k, v in batch.items()}\n","        model.zero_grad()\n","        outputs = model(**batch)\n","        loss, logits = outputs[:2]\n","        total_loss += loss.item()\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        scheduler.step()\n","        logits = logits.detach().cpu().numpy()\n","        predictions_labels += logits.argmax(axis=-1).flatten().tolist()\n","\n","    avg_epoch_loss = total_loss / len(dataloader)\n","\n","    return true_labels, predictions_labels, avg_epoch_loss\n","\n","\n","def validation(dataloader, device_):\n","    global model\n","\n","    predictions_labels = []\n","    true_labels = []\n","    total_loss = 0\n","\n","    model.eval()\n","\n","    for batch in tqdm(dataloader, total=len(dataloader)):\n","        true_labels += batch['labels'].numpy().flatten().tolist()\n","        batch = {k: v.type(torch.long).to(device_) for k, v in batch.items()}\n","        with torch.no_grad():\n","            outputs = model(**batch)\n","            loss, logits = outputs[:2]\n","            logits = logits.detach().cpu().numpy()\n","            total_loss += loss.item()\n","            predict_content = logits.argmax(axis=-1).flatten().tolist()\n","            predictions_labels += predict_content\n","    avg_epoch_loss = total_loss / len(dataloader)\n","    return true_labels, predictions_labels, avg_epoch_loss\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","print('Loading configuraiton...')\n","model_config = GPT2Config.from_pretrained(\n","    pretrained_model_name_or_path=model_name_or_path, num_labels=n_labels)\n","\n","print('Loading tokenizer...')\n","tokenizer = GPT2Tokenizer.from_pretrained(\n","    pretrained_model_name_or_path=model_name_or_path)\n","tokenizer.padding_side = \"left\"\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","print('Loading model...')\n","model = GPT2ForSequenceClassification.from_pretrained(\n","    pretrained_model_name_or_path=model_name_or_path, config=model_config)\n","\n","model.resize_token_embeddings(len(tokenizer))\n","\n","model.config.pad_token_id = model.config.eos_token_id\n","\n","model.to(device)\n","print('Model loaded to `%s`' % device)\n","\n","gpt2_classificaiton_collator = Gpt2ClassificationCollator(\n","    use_tokenizer=tokenizer, max_sequence_len=max_length)\n","\n","df_train, df_val = train_test_split(\n","    df,\n","    test_size=0.3,\n","    random_state=seed\n",")\n","# df_val, df_test = train_test_split(\n","#     df_val,\n","#     test_size=0.5,\n","#     random_state=seed\n","# )\n","\n","train_dataloader = create_data_loader(\n","    df_train, tokenizer, max_length, batch_size)\n","valid_dataloader = create_data_loader(\n","    df_val, tokenizer, max_length, batch_size)\n","# test_data_loader = create_data_loader(\n","#     df_test, tokenizer, max_length, batch_size)\n","\n","optimizer = AdamW(model.parameters(),\n","                  lr=2e-5,  # default is 5e-5, our notebook had 2e-5\n","                  eps=1e-8  # default is 1e-8.\n","                  )\n","\n","total_steps = len(train_dataloader) * epochs\n","\n","scheduler = get_linear_schedule_with_warmup(optimizer,\n","                                            num_warmup_steps=0,  # Default value in run_glue.py\n","                                            num_training_steps=total_steps)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["all_loss = {'train_loss': [], 'val_loss': []}\n","all_micro = {'train_micro': [], 'val_micro': []}\n","all_macro = {'train_macro': [], 'val_macro': []}\n","\n","print('Epoch')\n","for epoch in tqdm(range(epochs)):\n","    print()\n","    print('Training on batches...')\n","    train_labels, train_predict, train_loss = train(\n","        train_dataloader, optimizer, scheduler, device)\n","    train_micro = f1_score(train_labels, train_predict, average='micro')\n","    train_macro = f1_score(train_labels, train_predict, average='macro')\n","\n","    print(f'Train loss {train_loss} micro {train_micro} macro {train_macro}')\n","    print()\n","\n","    print('Validation on batches...')\n","    valid_labels, valid_predict, val_loss = validation(\n","        valid_dataloader, device)\n","    valid_micro = f1_score(valid_labels, valid_predict, average='micro')\n","    valid_macro = f1_score(valid_labels, valid_predict, average='macro')\n","\n","    print(f'Valid loss {val_loss} micro {valid_micro} macro {valid_macro}')\n","    print()\n","\n","    all_loss['train_loss'].append(train_loss)\n","    all_loss['val_loss'].append(val_loss)\n","    all_micro['train_micro'].append(train_micro)\n","    all_micro['val_micro'].append(valid_micro)\n","    all_macro['train_macro'].append(train_macro)\n","    all_macro['val_macro'].append(valid_macro)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["true_labels, predictions_labels, avg_epoch_loss = validation(\n","    valid_dataloader, device)\n","\n","evaluation_report = classification_report(true_labels, predictions_labels,)\n","print(evaluation_report)\n",""]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":2}}