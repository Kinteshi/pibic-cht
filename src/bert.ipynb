{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import string\n","import re\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torch import nn, optim\n","from textwrap import wrap\n","from collections import defaultdict\n","from sklearn.utils import resample\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import confusion_matrix, classification_report, f1_score\n","from sklearn.model_selection import train_test_split\n","from matplotlib import rc\n","import matplotlib.pyplot as plt\n","from pylab import rcParams\n","import seaborn as sns\n","import pandas as pd\n","import numpy as np\n","import torch\n","from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n","import time\n","from preprocessing.nlp import preprocess\n","from preprocessing.tratamento import filter_data\n","\n","RANDOM_SEED = 15\n","PRE_TRAINED_MODEL_NAME = 'neuralmind/bert-base-portuguese-cased'\n","MAX_LEN = 156\n","BATCH_SIZE = 16\n","EPOCHS = 10\n","\n","np.random.seed(RANDOM_SEED)\n","torch.manual_seed(RANDOM_SEED)\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","class TCEDataset(Dataset):\n","    def __init__(self, empenho, targets, tokenizer, max_len):\n","        self.empenho = empenho\n","        self.targets = targets\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.empenho)\n","\n","    def __getitem__(self, item):\n","        empenho = str(self.empenho[item])\n","        target = self.targets[item]\n","        encoding = self.tokenizer.encode_plus(\n","            empenho,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            return_token_type_ids=False,\n","            padding='max_length',\n","            return_attention_mask=True,\n","            return_tensors='pt',\n","            truncation=True\n","        )\n","        return {\n","            'empenho_text': empenho,\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","            'targets': torch.tensor(target, dtype=torch.long)\n","        }\n","\n","\n","def create_data_loader(df, tokenizer, max_len, batch_size):\n","    ds = TCEDataset(\n","        empenho=df.empenho.to_numpy(),\n","        targets=df.encodedNatureza.to_numpy(),\n","        tokenizer=tokenizer,\n","        max_len=max_len\n","    )\n","    return DataLoader(\n","        ds,\n","        batch_size=batch_size,\n","    )\n","\n","\n","class NaturezaClassifier(nn.Module):\n","    def __init__(self, n_classes):\n","        super(NaturezaClassifier, self).__init__()\n","        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n","        self.drop = nn.Dropout(p=0.3)\n","        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n","\n","    def forward(self, input_ids, attention_mask):\n","        bert_output = self.bert(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask\n","        )\n","\n","        output = self.drop(bert_output['pooler_output'])\n","        return self.out(output)\n","\n","\n","def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n","    model = model.train()\n","    losses = []\n","    correct_predictions = 0\n","    predictions = []\n","    real_values = []\n","    for d in data_loader:\n","        input_ids = d[\"input_ids\"].to(device)\n","        attention_mask = d[\"attention_mask\"].to(device)\n","        targets = d[\"targets\"].to(device)\n","        outputs = model(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask\n","        )\n","        _, preds = torch.max(outputs, dim=1)\n","        loss = loss_fn(outputs, targets)\n","        correct_predictions += torch.sum(preds == targets)\n","        losses.append(loss.item())\n","        loss.backward()\n","        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","        optimizer.step()\n","        scheduler.step()\n","        optimizer.zero_grad()\n","        predictions.extend(preds)\n","        real_values.extend(targets)\n","    predictions = torch.stack(predictions).cpu()\n","    real_values = torch.stack(real_values).cpu()\n","    macro = f1_score(real_values, predictions, average='macro')\n","    micro = f1_score(real_values, predictions, average='micro')\n","    return correct_predictions.double() / n_examples, np.mean(losses), macro, micro\n","\n","\n","def eval_model(model, data_loader, loss_fn, device, n_examples):\n","    model = model.eval()\n","    losses = []\n","    correct_predictions = 0\n","    predictions = []\n","    real_values = []\n","    with torch.no_grad():\n","        for d in data_loader:\n","            input_ids = d[\"input_ids\"].to(device)\n","            attention_mask = d[\"attention_mask\"].to(device)\n","            targets = d[\"targets\"].to(device)\n","            outputs = model(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask\n","            )\n","            _, preds = torch.max(outputs, dim=1)\n","            loss = loss_fn(outputs, targets)\n","            correct_predictions += torch.sum(preds == targets)\n","            losses.append(loss.item())\n","            predictions.extend(preds)\n","            real_values.extend(targets)\n","    predictions = torch.stack(predictions).cpu()\n","    real_values = torch.stack(real_values).cpu()\n","    macro = f1_score(real_values, predictions, average='macro')\n","    micro = f1_score(real_values, predictions, average='micro')\n","    return correct_predictions.double() / n_examples, np.mean(losses), macro, micro\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df = filter_data(\n","    'C:\\\\Users\\\\jefma\\\\OneDrive\\\\Documentos\\\\GitHub\\\\pibic-cht\\\\data\\\\dadosTCE.csv',\n","    'C:\\\\Users\\\\jefma\\\\OneDrive\\\\Documentos\\\\GitHub\\\\pibic-cht\\\\data\\\\norel.xlsx'\n",")\n","df = df[['empenho_historico', 'natureza_despesa_cod']]\n","df.columns = ['empenho', 'natureza']\n","df.empenho = df.empenho.apply(preprocess)\n","\n","lb = LabelEncoder()\n","df['encodedNatureza'] = lb.fit_transform(df.natureza)\n","\n","tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n","\n","df_train, df_test = train_test_split(\n","    df,\n","    test_size=0.3,\n","    random_state=RANDOM_SEED,\n","    stratify=df.natureza\n",")\n","df_val, df_test = train_test_split(\n","    df_test,\n","    test_size=0.5,\n","    random_state=RANDOM_SEED,\n","    stratify=df_test.natureza\n",")\n","\n","train_data_loader = create_data_loader(\n","    df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n","val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n","test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = NaturezaClassifier(len(lb.classes_))\n","model = model.to(device)\n","\n","# model.load_state_dict(torch.load('best_model_state.bin', map_location=torch.device(device)))\n","\n","optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n","total_steps = len(train_data_loader) * EPOCHS\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer,\n","    num_warmup_steps=0,\n","    num_training_steps=total_steps\n",")\n","loss_fn = nn.CrossEntropyLoss().to(device)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["history = defaultdict(list)\n","best_accuracy = 0\n","for epoch in range(EPOCHS):\n","    starting = time.time()\n","    print(f'Epoch {epoch + 1}/{EPOCHS}')\n","    print('-' * 10)\n","    train_acc, train_loss, train_macro, train_micro = train_epoch(\n","        model,\n","        train_data_loader,\n","        loss_fn,\n","        optimizer,\n","        device,\n","        scheduler,\n","        len(df_train)\n","    )\n","    print(\n","        f'Train loss {train_loss} accuracy {train_acc} macro {train_macro} micro {train_micro}')\n","    val_acc, val_loss, val_macro, val_micro = eval_model(\n","        model,\n","        val_data_loader,\n","        loss_fn,\n","        device,\n","        len(df_val)\n","    )\n","    print(\n","        f'Val   loss {val_loss} accuracy {val_acc} macro {val_macro} micro {val_micro}')\n","    print()\n","    history['train_acc'].append(train_acc)\n","    history['train_loss'].append(train_loss)\n","    history['train_macro'].append(train_macro)\n","    history['train_micro'].append(train_micro)\n","    history['val_acc'].append(val_acc)\n","    history['val_loss'].append(val_loss)\n","    history['val_macro'].append(val_macro)\n","    history['val_micro'].append(val_micro)\n","    if val_acc > best_accuracy:\n","        torch.save(model.state_dict(), 'best_model_state.bin')\n","        best_accuracy = val_acc\n","    print(f'{(time.time()-starting)/60}')\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","plt.plot(history['train_acc'], label='train accuracy')\n","plt.plot(history['val_acc'], label='validation accuracy')\n","plt.title('Training history')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend()\n","plt.ylim([0, 1])\n","plt.savefig('Acc.png')\n","\n","plt.cla()\n","plt.plot(history['train_loss'], label='train loss')\n","plt.plot(history['val_loss'], label='validation loss')\n","plt.title('Training history')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend()\n","plt.ylim([0, 1])\n","plt.savefig('Loss.png')\n","\n","plt.cla()\n","plt.plot(history['train_macro'], label='train macro')\n","plt.plot(history['val_macro'], label='validation macro')\n","plt.title('Training history')\n","plt.ylabel('Macro')\n","plt.xlabel('Epoch')\n","plt.legend()\n","plt.ylim([0, 1])\n","plt.savefig('Macro.png')\n","\n","plt.cla()\n","plt.plot(history['train_micro'], label='train micro')\n","plt.plot(history['val_micro'], label='validation micro')\n","plt.title('Training history')\n","plt.ylabel('Micro')\n","plt.xlabel('Epoch')\n","plt.legend()\n","plt.ylim([0, 1])\n","plt.savefig('Micro.png')\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_acc, _, _, _ = eval_model(\n","    model,\n","    test_data_loader,\n","    loss_fn,\n","    device,\n","    len(df_test)\n",")\n","print(test_acc.item())\n","\n","\n","def get_predictions(model, data_loader):\n","    model = model.eval()\n","    review_texts = []\n","    predictions = []\n","    prediction_probs = []\n","    real_values = []\n","    with torch.no_grad():\n","        for d in data_loader:\n","            texts = d[\"empenho_text\"]\n","            input_ids = d[\"input_ids\"].to(device)\n","            attention_mask = d[\"attention_mask\"].to(device)\n","            targets = d[\"targets\"].to(device)\n","            outputs = model(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask\n","            )\n","            _, preds = torch.max(outputs, dim=1)\n","            review_texts.extend(texts)\n","            predictions.extend(preds)\n","            prediction_probs.extend(outputs)\n","            real_values.extend(targets)\n","    predictions = torch.stack(predictions).cpu()\n","    prediction_probs = torch.stack(prediction_probs).cpu()\n","    real_values = torch.stack(real_values).cpu()\n","    return review_texts, predictions, prediction_probs, real_values\n","\n","\n","y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n","    model,\n","    test_data_loader\n",")\n","\n","print('Test Classification Report')\n","print(classification_report(y_test, y_pred))\n",""]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":2}}